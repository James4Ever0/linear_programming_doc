{
    "summary": "The code trains a neural network model using inputs and outputs, defines the MSE loss function, and employs Adam optimizer. It performs forward pass and backpropagation for training, prints loss every 100 iterations, zeroes gradients before backward pass, and uses linear layer parameters as weight and bias.",
    "details": [
        {
            "comment": "Code creates inputs and outputs for neural network training, defines model, loss function (MSE), optimizer (Adam), and trains the model using forward pass and backpropagation.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/neural_network_linearize_nonlinear_function_test.py\":0-38",
            "content": "import numpy as np\nfrom linearization_config import *\n# expression: z = x * sin(y)\nx = np.linspace(x_lb, x_ub, x_sample_size)\ny = np.linspace(y_lb, y_ub, y_sample_size)\nz = outputs = np.array(\n    [z_func(x_element, y_element) for y_element in y for x_element in x]\n)\ninputs = np.array([[x_element, y_element] for y_element in y for x_element in x])\nimport torch\nfrom torch import Tensor\nfrom neural_network_demo import model\nprint(model)\ninputs_tensor = Tensor(inputs)\noutputs_tensor = Tensor(outputs.reshape(-1, 1))\nprint(inputs_tensor.shape, outputs_tensor.shape)\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nlearning_rate = 1e-4\ntrain_epoches = 30000\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = torch.nn.MSELoss(reduction=\"sum\")\nfor t in range(train_epoches):\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When"
        },
        {
            "comment": "The code trains a neural network model, computes the loss, performs backpropagation, and saves the trained model. It prints the loss every 100 iterations, zeroes gradients before the backward pass, and uses a linear layer for calculations. The linear layer's parameters are stored as weight and bias.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/neural_network_linearize_nonlinear_function_test.py\":39-65",
            "content": "    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(inputs_tensor)\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, outputs_tensor)\n    if t % 100 == 99:\n        print(t, loss.item())\n    # Zero the gradients before running the backward pass.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\nprint(\"TRAINING COMPLETE\")\n# You can access the first layer of `model` like accessing the first item of a list\n# bias_1 = input_layer.bias\n# weight_1 = input_layer.weight\nmodel_save_path = \"model.pt\"\ntorch.save(model.state_dict(), model_save_path)\n# breakpoint()\n# # For linear layer, its parameters are stored as `weight` and `bias`.\n# print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
        }
    ]
}