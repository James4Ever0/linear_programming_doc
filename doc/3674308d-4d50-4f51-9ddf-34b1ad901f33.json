{
    "summary": "The code imports libraries, checks OS, defines a function to repair Excel files, parses and extracts data, filters unwanted rows, handles duplicates, updates target_json, interacts with Excel and CSVs, logs row info for debugging, adds values to result dictionary, logs and saves JSON file.",
    "details": [
        {
            "comment": "This code imports necessary libraries, checks the operating system, and defines a function to repair Excel files. It uses different methods for Windows (Win32com) and macOS (LibreOffice) to convert potentially damaged Excel files into usable format, ensuring compatibility with other parts of the program.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":0-34",
            "content": "from log_utils import logger_print\n# first get the titles.\n# \u89e3\u6790\u8bbe\u5907\u53c2\u6570\u8868 \u53ef\u80fd\u4e5f\u9002\u7528\u4e8e\u8bbe\u5907\u63a5\u53e3\u4fe1\u606f\u89e3\u6790\nimport openpyxl\nfrom openpyxl.worksheet.worksheet import Worksheet\nimport pandas\nimport rich\nimport numpy\nimport json\nimport os\nif os.name == \"nt\":\n    from win32com.client import Dispatch\n    def repair_excel(excel_path):  # you may need to restart system if this goes wrong.\n        xlapp = Dispatch(\"ket.Application\")  # wps\n        # xlapp = Dispatch(\"Excel.Application\")\n        xlapp.Visible = False\n        xlbook = xlapp.Workbooks.Open(os.path.abspath(excel_path))\n        xlbook.Save()\n        xlbook.Close()\nelse:\n    # in macos\n    def repair_excel(excel_path):\n        import tempfile\n        soffice_bin = \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n        with tempfile.TemporaryDirectory() as TD:\n            tmpdir = os.path.abspath(TD)\n            excel_path_abs = os.path.abspath(excel_path)\n            excel_path_base = os.path.basename(excel_path)\n            commandline = f\"'{soffice_bin}' --headless --convert-to 'xlsx:{excel_path_base.split('.')[0]}' --outdir '{tmpdir}' '{excel_path_abs}'\""
        },
        {
            "comment": "The code appears to be part of a program that processes data from an Excel sheet. It defines functions for parsing and converting elements, extracts a specific sheet from the Excel file based on input parameters, and retrieves column ranges within rows. The program may use these functions to manipulate or process the extracted data further.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":35-67",
            "content": "            os.system(commandline)\n            os.system(f\"mv {os.path.join(tmpdir, excel_path_base)} {excel_path_abs}\")\ndef strip_and_convert_empty_string_as_none(e: str):\n    stripped = e.strip()\n    if stripped == \"\":\n        return None\n    else:\n        return stripped\ndef strip_element_if_is_string(mlist):\n    return [\n        e if not isinstance(e, str) else strip_and_convert_empty_string_as_none(e)\n        for e in mlist\n    ]\ndef main_parser(filepath, sheet_name, output_path, type_utils_parser: bool):\n    # if os.name == \"nt\":\n    sheet1 = extract_sheet_from_excel(filepath, sheet_name)\n    # common parts?\n    # order: category; name (unit), example, delete or not\n    # you need to scan through all cells to find some cell with specific color.\n    # and with some example.\n    # COL: A;B,C,D;F,G,H for all data need to export\n    # after (partial) serialization, you can do something more interesting with it.\n    dims = sheet1.row_dimensions, sheet1.column_dimensions\n    uniqs = {}\n    def getColumnRangePerRow(start, end):"
        },
        {
            "comment": "This code parses an Excel sheet, specifically columns headers and data rows. It checks if certain headers are present or not. It creates a dictionary where keys are row indexes and values are column headers. If type_utils_parser is True, it filters out some unwanted headers. Then it creates a target JSON object using the filtered column names. Lastly, it extracts data from specific columns (device name, device ports, details) and separates them into separate lists without indices.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":68-103",
            "content": "        flag = True\n        for index, row in enumerate(sheet1.rows):\n            if flag:\n                flag = False\n                continue\n            yield index, [col.value for col in row[start:end]]\n    heads = getColumnRangePerRow(0, 1)\n    unwanted_headers = [\"\u5176\u4ed6\"]\n    # cursor = None\n    headMaps = {}\n    prevHead = None\n    mHeads = []\n    for index, [head] in heads:\n        logger_print(index, head)\n        if head:\n            prevHead = head\n            mHeads.append(head)\n        if prevHead:\n            headMaps.update({index: prevHead})\n    logger_print(headMaps)\n    target_json = {\n        h: {}\n        for h in mHeads\n        if (True if type_utils_parser is False else h not in unwanted_headers)\n    }\n    if type_utils_parser:\n        # breakpoint()\n        deviceNameAndPorts = list(getColumnRangePerRow(1, 3))\n        deviceNameAndPortsWithoutIndex = [row for _, row in deviceNameAndPorts]\n        details = list(getColumnRangePerRow(4, 10))\n        indexs = [i for i, _ in details]\n        details_without_index = [row for _, row in details]"
        },
        {
            "comment": "The code is iterating through rows and headers, skipping any rows with unwanted headers. It extracts device name or port name along with other details such as type, basic type, direction, must-have scenarios, associated rules, and additional requirements from the rows. If a current device name exists, it checks if the row contains data for that device, and if not, sets the current device name to None. It also creates a new device data template if needed.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":104-132",
            "content": "        # breakpoint()\n        currentDevName = None\n        currentDevData = None\n        prevDevName = None\n        createDevDataTemplate = lambda: {\n            \"ports\": dict(),\n            \"rules\": [],\n            \"requirements\": [],\n        }\n        for i, row_i in enumerate(indexs):\n            head = headMaps[row_i]\n            if head in unwanted_headers:\n                logger_print(\n                    f'skipping row #{row_i} because of \"{head}\" is in unwanted headers ({unwanted_headers}).'\n                )\n                continue\n            devNameOrPortName, portInfo = strip_element_if_is_string(\n                deviceNameAndPortsWithoutIndex[i]\n            )\n            \u7ec6\u5206\u7c7b\u578b, \u57fa\u672c\u7c7b\u578b, \u80fd\u6d41\u65b9\u5411, \u5fc5\u6709\u5de5\u51b5, \u5bf9\u5e94\u89c4\u5219, \u9644\u52a0\u8981\u6c42 = strip_element_if_is_string(\n                details_without_index[i]\n            )\n            if currentDevName is not None:\n                if devNameOrPortName is None:\n                    currentDevName = None\n                else:\n                    if currentDevData is None:\n                        currentDevData = createDevDataTemplate()"
        },
        {
            "comment": "The code is parsing parameters for devices in a microgrid. It checks for duplicate port names and adds new device data to the existing one if it exists, otherwise, creates a new entry. If rules or requirements are present, they are appended accordingly.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":133-152",
            "content": "                    portName = devNameOrPortName\n                    assert (\n                        portName not in currentDevData[\"ports\"].keys()\n                    ), f\"duplicate port name found: {currentDevName} -> {portName}\"\n                    currentDevData[\"ports\"][portName] = dict(\n                        info=portInfo, \u7ec6\u5206\u7c7b\u578b=\u7ec6\u5206\u7c7b\u578b, \u57fa\u672c\u7c7b\u578b=\u57fa\u672c\u7c7b\u578b, \u80fd\u6d41\u65b9\u5411=\u80fd\u6d41\u65b9\u5411, \u5fc5\u6709\u5de5\u51b5=\u5fc5\u6709\u5de5\u51b5\n                    )\n                    if \u5bf9\u5e94\u89c4\u5219 is not None:\n                        currentDevData[\"rules\"].append(\u5bf9\u5e94\u89c4\u5219)\n                    if \u9644\u52a0\u8981\u6c42 is not None:\n                        currentDevData[\"requirements\"].append(\u9644\u52a0\u8981\u6c42)\n            else:\n                if currentDevData is not None:\n                    prevHead2 = headMaps[row_i - 1]\n                    target_json[prevHead2][prevDevName] = currentDevData\n                    currentDevData = None\n                if portInfo is None:\n                    if devNameOrPortName is not None:\n                        currentDevName = devNameOrPortName\n                        prevDevName = currentDevName"
        },
        {
            "comment": "This code defines a function that processes BCD (Big Column Data) by checking if each element is empty or not, and updates target_json based on the device name and corresponding values. The processBCD() function iterates through the BCD columns, checks for empty elements, logs LINE BREAK when all elements are empty, and updates the device name and its corresponding values in target_json.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":154-181",
            "content": "    else:\n        BCD = getColumnRangePerRow(1, 4)\n        FGH = getColumnRangePerRow(5, 8)\n        def checkEmpty(val):\n            if type(val) == str:\n                if val.strip() == \"\":\n                    return None\n            return val\n        def processBCD(_BCD):\n            device_name = None\n            for index, [b, c, d] in _BCD:\n                head = headMaps[index]\n                b, c, d = checkEmpty(b), checkEmpty(c), checkEmpty(d)\n                # logger_print(b,c,d) # value can be None or \"\"\n                if all([elem is None for elem in [b, c, d]]):\n                    logger_print(\"LINE BREAK\")\n                    device_name = None\n                else:\n                    if device_name is None:\n                        device_name = b\n                        target_json[head].update({device_name: []})\n                    else:\n                        target_json[head][device_name].append((b, c, d))\n                    logger_print(\"DEVICE NAME?\", device_name)\n        processBCD(BCD)"
        },
        {
            "comment": "The code snippet is a part of a larger program that interacts with Excel files and CSVs. The `extract_sheet_from_excel` function loads an Excel file (`filepath`) and returns the specified sheet (`sheet_name`). It also checks if the sheet is indeed a Worksheet type before returning it. The `csv_parser` function reads a CSV file (`filename`), and iterates over each row in the DataFrame, logging row data for potential debugging purposes.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":182-217",
            "content": "        processBCD(FGH)\n    logger_print(target_json)\n    with open(output_path, \"w+\") as f:\n        f.write(json.dumps(target_json, indent=4, ensure_ascii=False))\n    logger_print(\"WRITE TO:\", output_path)\ndef extract_sheet_from_excel(filepath, sheet_name):\n    repair_excel(filepath)\n    excel_file = openpyxl.load_workbook(filepath)\n    # excel_file = openpyxl.load_workbook(filepath, read_only=True)\n    logger_print(\"SHEET NAMES:\")\n    logger_print(excel_file.sheetnames)  # ['Sheet1']\n    # from openpyxl.cell.cell import Cell, MergedCell\n    sheet1 = excel_file[sheet_name]\n    if not isinstance(sheet1, Worksheet):\n        raise Exception(\n            f\"sheet {sheet_name} at file '{filepath}' (type: {type(sheet1)}) is not Worksheet\"\n        )\n    return sheet1\ndef csv_parser(filename, output_path):\n    df = pandas.read_csv(filename, header=None)\n    dataClasses = [None, None]\n    result = {}\n    lastEmpty = True\n    for index, row in df.iterrows():\n        # logger_print(row)\n        # logger_print(list(row))\n        list_row = list(row)"
        },
        {
            "comment": "The code checks if the first and second elements of a list_row are both NaN, then continues. If the first element is a string, it strips any leading/trailing spaces and adds it to dataClasses[0]. If the second element is also a string with non-empty value, and lastEmpty is True, it adds it to dataClasses[1] and sets lastEmpty to False. If both elements are non-empty strings, it checks if result already has dataClasses[0] key; if not, creates an empty dictionary for that key. Then, it checks if the dictionary with key dataClasses[0] has a subkey with dataClasses[1], if not, creates one.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":218-241",
            "content": "        first, second = list_row[:2]\n        # logger_print(dir(row))\n        if first is numpy.nan and second is numpy.nan:\n            lastEmpty = True\n            continue\n        # list_row_types = [(e, type(e)) for e in list_row]\n        # logger_print(list_row_types)\n        # numpy.nan is a float, not an int, so we can't use it as a number\n        if type(first) == str:\n            first = first.strip()\n            if len(first) > 0:\n                dataClasses[0] = first\n        if type(second) == str:\n            second = second.strip()\n            if len(second) > 0:\n                if lastEmpty:\n                    dataClasses[1] = second\n                    lastEmpty = False\n                else:\n                    # now we begin to insert data.\n                    if dataClasses[0] and dataClasses[1]:\n                        if result.get(dataClasses[0], None) is None:\n                            result[dataClasses[0]] = {}\n                        if result[dataClasses[0]].get(dataClasses[1], None) is None:"
        },
        {
            "comment": "The code adds an empty list to the result dictionary at the specified data classes indices, appends a value (second) to it, logs and saves the resulting dictionary as a formatted JSON file.",
            "location": "\"/media/root/Prima/works/generated_docs/linear_programming_doc/src/microgrid_base/lib_parse_params.py\":242-246",
            "content": "                            result[dataClasses[0]][dataClasses[1]] = []\n                        result[dataClasses[0]][dataClasses[1]].append(second)\n    logger_print(result)\n    with open(output_path, \"w+\") as f:\n        f.write(json.dumps(result, indent=4, ensure_ascii=False))"
        }
    ]
}